{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpellChecker.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadAwaisAhmad/Spell-Checker/blob/master/SpellChecker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw1V9sVHMYVD",
        "colab_type": "text"
      },
      "source": [
        "# Spell Checker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLmDwYN5MYVH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8b37fd7a-e097-4b09-f138-bbe3f7e3acd5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from collections import namedtuple\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "import time\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hm1gaIFMYVL",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aINRDxcjMYVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the books using the file names\n",
        "books  = open('Books.txt').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5tTITIOMYVa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d9c2da47-a8a1-484a-d325-bff9cb2ea294"
      },
      "source": [
        "# Check to ensure the text looks alright\n",
        "books[:500]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffProject Gutenberg’s Alice’s Adventures in Wonderland, by Lewis Carroll\\n\\nThis eBook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\nre-use it under the terms of the Project Gutenberg License included\\nwith this eBook or online at www.gutenberg.org\\n\\n\\nTitle: Alice’s Adventures in Wonderland\\n\\nAuthor: Lewis Carroll\\n\\nPosting Date: June 25, 2008 [EBook #11]\\nRelease Date: March, 1994\\nLast Updated: October 6, 2016\\n\\nLanguage: Engli'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4beIkJx8MYVf",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iySkfemLMYVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Remove unwanted characters and extra spaces from the text'''\n",
        "    text = re.sub(r'\\n', ' ', text) \n",
        "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n",
        "    text = re.sub('a0','', text)\n",
        "    text = re.sub('\\'92t','\\'t', text)\n",
        "    text = re.sub('\\'92s','\\'s', text)\n",
        "    text = re.sub('\\'92m','\\'m', text)\n",
        "    text = re.sub('\\'92ll','\\'ll', text)\n",
        "    text = re.sub('\\'91','', text)\n",
        "    text = re.sub('\\'92','', text)\n",
        "    text = re.sub('\\'93','', text)\n",
        "    text = re.sub('\\'94','', text)\n",
        "    text = re.sub('\\.','. ', text)\n",
        "    text = re.sub('\\!','! ', text)\n",
        "    text = re.sub('\\?','? ', text)\n",
        "    text = re.sub(' +',' ', text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sln_WDWRMYVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean the text of the books\n",
        "clean_books = clean_text(books)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFe3x8jvMYVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "09ea0a30-f177-49fb-f98e-9a1b5aa6de29"
      },
      "source": [
        "# Check to ensure the text has been cleaned properly\n",
        "clean_books[:500]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffProject Gutenberg’s Alice’s Adventures in Wonderland, by Lewis Carroll This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www. gutenberg. org Title: Alice’s Adventures in Wonderland Author: Lewis Carroll Posting Date: June 25, 2008 EBook 11 Release Date: March, 1994 Last Updated: October 6, 2016 Language: English Chara'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oksy3Z24MYVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary to convert the vocabulary (characters) to integers\n",
        "vocab_to_int = {}\n",
        "count = 0\n",
        "for character in clean_books:\n",
        "  if character not in vocab_to_int:\n",
        "    vocab_to_int[character] = count\n",
        "    count += 1\n",
        "\n",
        "# Add special tokens to vocab_to_int\n",
        "codes = ['<PAD>','<EOS>','<GO>']\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = count\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSIpVUhDMYVr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "94bbc7f3-8ffe-4c21-a95d-b88bc485e473"
      },
      "source": [
        "# Check the size of vocabulary and all of the values\n",
        "vocab_size = len(vocab_to_int)\n",
        "print(\"The vocabulary contains {} characters.\".format(vocab_size))\n",
        "print(sorted(vocab_to_int))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocabulary contains 112 characters.\n",
            "[' ', '!', '\"', '$', '&', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<EOS>', '<GO>', '<PAD>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'À', 'Æ', 'Ç', 'É', 'à', 'á', 'â', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ï', 'ô', 'ö', 'ü', 'Œ', 'œ', '–', '—', '―', '‘', '’', '“', '”', '†', '™', '\\ufeff', '�']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu5K27MfVKSy",
        "colab_type": "text"
      },
      "source": [
        "**Convert number to vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO6RTgvxMYVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create another dictionary to convert integers to their respective characters\n",
        "int_to_vocab = {}\n",
        "for character, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = character"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNYZCPoAMYVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c55503d8-3de0-4560-d61b-3fd8ca5396a5"
      },
      "source": [
        "# Split the text from the books into sentences.\n",
        "sentences = []\n",
        "sentences = nltk.sent_tokenize(clean_books)\n",
        "print(\"There are {} sentences.\".format(len(sentences)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 152042 sentences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItSdWXTUMYV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6cf954aa-ab4a-4f20-ad9c-2d135fb84348"
      },
      "source": [
        "# Check to ensure the text has been split correctly.\n",
        "sentences[:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\ufeffProject Gutenberg’s Alice’s Adventures in Wonderland, by Lewis Carroll This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.',\n",
              " 'You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.',\n",
              " 'gutenberg.',\n",
              " 'org Title: Alice’s Adventures in Wonderland Author: Lewis Carroll Posting Date: June 25, 2008 EBook 11 Release Date: March, 1994 Last Updated: October 6, 2016 Language: English Character set encoding: UTF-8 START OF THIS PROJECT GUTENBERG EBOOK ALICE’S ADVENTURES IN WONDERLAND ALICE’S ADVENTURES IN WONDERLAND Lewis Carroll THE MILLENNIUM FULCRUM EDITION 3.',\n",
              " '0 CHAPTER I.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpS_H-uxMYV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert sentences to integers\n",
        "int_sentences = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    int_sentence = []\n",
        "    for character in sentence:\n",
        "        int_sentence.append(vocab_to_int[character])\n",
        "    int_sentences.append(int_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKDmPk6wMYWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the length of each sentence\n",
        "lengths = []\n",
        "for sentence in int_sentences:\n",
        "    lengths.append(len(sentence))\n",
        "lengths = pd.DataFrame(lengths, columns=[\"counts\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_x3t0zRMYWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "f039127a-c633-4539-b13e-8268ad683185"
      },
      "source": [
        "lengths.describe()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>152042.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>105.547119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>106.620696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>35.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>77.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>143.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>8906.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              counts\n",
              "count  152042.000000\n",
              "mean      105.547119\n",
              "std       106.620696\n",
              "min         1.000000\n",
              "25%        35.000000\n",
              "50%        77.000000\n",
              "75%       143.000000\n",
              "max      8906.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFC_e-dXMYWO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ba7727c-a469-4871-d479-02aed42b7ad3"
      },
      "source": [
        "# Limit the data we will use to train our model\n",
        "max_length = 300\n",
        "min_length = 10\n",
        "\n",
        "good_sentences = []\n",
        "\n",
        "for sentence in int_sentences:\n",
        "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
        "        good_sentences.append(sentence)\n",
        "\n",
        "print(\"We will use {} to train and test our model.\".format(len(good_sentences)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We will use 136961 to train and test our model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBAz97BBMYWR",
        "colab_type": "text"
      },
      "source": [
        "*Note:  not using very long or short sentences because they are not as useful for training our model. Shorter sentences are less likely to include an error and the text is more likely to be repetitive. Longer sentences are more difficult to learn due to their length and increase the training time quite a bit.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpHDW5eNMYWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8d88bd6d-82a5-4118-f5a8-d5f08ad898c9"
      },
      "source": [
        "# Split the data into training and testing sentences\n",
        "training, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 2)\n",
        "\n",
        "print(\"Number of training sentences:\", len(training))\n",
        "print(\"Number of testing sentences:\", len(testing))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 116416\n",
            "Number of testing sentences: 20545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzKJcNA6MYWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort the sentences by length to reduce padding, which will allow the model to train faster\n",
        "training_sorted = []\n",
        "testing_sorted = []\n",
        "\n",
        "for i in range(min_length, max_length+1):\n",
        "    for sentence in training:\n",
        "        if len(sentence) == i:\n",
        "            training_sorted.append(sentence)\n",
        "    for sentence in testing:\n",
        "        if len(sentence) == i:\n",
        "            testing_sorted.append(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqelr722MYWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "efbca3f6-fe89-429f-aeca-aeb79c0e38e7"
      },
      "source": [
        "# Check to ensure the sentences have been selected and sorted correctly\n",
        "for i in range(5):\n",
        "    print(training_sorted[i], len(training_sorted[i]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[13, 10, 7, 5, 11, 12, 5, 2, 13, 34] 10\n",
            "[13, 10, 7, 5, 11, 12, 5, 2, 13, 34] 10\n",
            "[71, 8, 15, 22, 18, 19, 8, 29, 5, 34] 10\n",
            "[28, 5, 17, 17, 8, 33, 5, 34, 8, 95] 10\n",
            "[71, 8, 70, 56, 3, 26, 8, 15, 3, 63] 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBQ7rk3dMYWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
        "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
        "\n",
        "def noise_maker(sentence, threshold):\n",
        "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
        "    \n",
        "    noisy_sentence = []\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        random = np.random.uniform(0,1,1)\n",
        "        # Most characters will be correct since the threshold value is high\n",
        "        if random < threshold:\n",
        "            noisy_sentence.append(sentence[i])\n",
        "        else:\n",
        "            new_random = np.random.uniform(0,1,1)\n",
        "            # ~33% chance characters will swap locations\n",
        "            if new_random > 0.67:\n",
        "                if i == (len(sentence) - 1):\n",
        "                    # If last character in sentence, it will not be typed\n",
        "                    continue\n",
        "                else:\n",
        "                    # if any other character, swap order with following character\n",
        "                    noisy_sentence.append(sentence[i+1])\n",
        "                    noisy_sentence.append(sentence[i])\n",
        "                    i += 1\n",
        "            # ~33% chance an extra lower case letter will be added to the sentence\n",
        "            elif new_random < 0.33:\n",
        "                random_letter = np.random.choice(letters, 1)[0]\n",
        "                noisy_sentence.append(vocab_to_int[random_letter])\n",
        "                noisy_sentence.append(sentence[i])\n",
        "            # ~33% chance a character will not be typed\n",
        "            else:\n",
        "                pass     \n",
        "        i += 1\n",
        "    return noisy_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAhF1-4aMYWk",
        "colab_type": "text"
      },
      "source": [
        "*Note: The noise_maker function is used to create spelling mistakes that are similar to those we would make. Sometimes we forget to type a letter, type a letter in the wrong location, or add an extra letter.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKKO27Z5MYWk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f81c94c5-2a19-4551-dfd5-76dd0dbfdbac"
      },
      "source": [
        "# Check to ensure noise_maker is making mistakes correctly.\n",
        "threshold = 0.9\n",
        "for sentence in training_sorted[:5]:\n",
        "    print(sentence)\n",
        "    print(noise_maker(sentence, threshold))\n",
        "    print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[13, 10, 7, 5, 11, 12, 5, 2, 13, 34]\n",
            "[13, 7, 5, 11, 12, 5, 2, 13, 34]\n",
            "\n",
            "[13, 10, 7, 5, 11, 12, 5, 2, 13, 34]\n",
            "[13, 10, 7, 5, 12, 5, 2, 13, 34]\n",
            "\n",
            "[71, 8, 15, 22, 18, 19, 8, 29, 5, 34]\n",
            "[8, 15, 22, 18, 19, 8, 17, 29, 5, 33, 34]\n",
            "\n",
            "[28, 5, 17, 17, 8, 33, 5, 34, 8, 95]\n",
            "[28, 5, 17, 3, 17, 8, 5, 5, 34, 95, 8]\n",
            "\n",
            "[71, 8, 70, 56, 3, 26, 8, 15, 3, 63]\n",
            "[15, 71, 8, 70, 56, 3, 26, 8, 15, 3, 63]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "FIMrGUinMYWo",
        "colab_type": "text"
      },
      "source": [
        "# Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KUcdn2xMYWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    with tf.name_scope('inputs'):\n",
        "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "    with tf.name_scope('targets'):\n",
        "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n",
        "    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n",
        "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
        "\n",
        "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlX_dhBLMYWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_encoding_input(targets, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    with tf.name_scope(\"process_encoding\"):\n",
        "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
        "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob6I-Oq3MYWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    if direction == 1:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "\n",
        "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                         input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n",
        "                                                              rnn_inputs,\n",
        "                                                              sequence_length,\n",
        "                                                              dtype=tf.float32)\n",
        "\n",
        "            return enc_output, enc_state\n",
        "        \n",
        "        \n",
        "    if direction == 2:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                            input_keep_prob = keep_prob)\n",
        "\n",
        "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                            input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                            cell_bw, \n",
        "                                                                            rnn_inputs,\n",
        "                                                                            sequence_length,\n",
        "                                                                            dtype=tf.float32)\n",
        "            # Join outputs since we are using a bidirectional RNN\n",
        "            enc_output = tf.concat(enc_output,2)\n",
        "            # Use only the forward state because the model can't use both states at once\n",
        "            return enc_output, enc_state[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i87dzwUmMYWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_target_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    with tf.name_scope(\"Training_Decoder\"):\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                            sequence_length=targets_length,\n",
        "                                                            time_major=False)\n",
        "\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                           training_helper,\n",
        "                                                           initial_state,\n",
        "                                                           output_layer) \n",
        "\n",
        "        training_logits, _,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                               output_time_major=False,\n",
        "                                                               impute_finished=True,\n",
        "                                                               maximum_iterations=max_target_length)\n",
        "        return training_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUYs9oRUMYW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_target_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    with tf.name_scope(\"Inference_Decoder\"):\n",
        "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                    start_tokens,\n",
        "                                                                    end_token)\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                            inference_helper,\n",
        "                                                            initial_state,\n",
        "                                                            output_layer)\n",
        "\n",
        "        inference_logits, _,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                                output_time_major=False,\n",
        "                                                                impute_finished=True,\n",
        "                                                                maximum_iterations=max_target_length)\n",
        "\n",
        "        return inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rcbtD7sMYW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
        "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
        "        for layer in range(num_layers):\n",
        "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                         input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  inputs_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "    \n",
        "    with tf.name_scope(\"Attention_Wrapper\"):\n",
        "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                              attn_mech,\n",
        "                                                              rnn_size)\n",
        "    \n",
        "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state,\n",
        "    #                                                                _zero_state_tensors(rnn_size, \n",
        "    #                                                                                    batch_size, \n",
        "    #                                                                                   tf.float32))\n",
        "    initial_state = dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
        "    #initial_state = initial_state.clone(cell_state=encoder_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input, \n",
        "                                                  targets_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_target_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_target_length,\n",
        "                                                    batch_size)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-5_UbdTMYW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n",
        "                                           enc_embed_input, keep_prob, direction)\n",
        "    \n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        dec_embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        inputs_length, \n",
        "                                                        targets_length, \n",
        "                                                        max_target_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers,\n",
        "                                                        direction)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zCNopBMMYW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGcHO3ODMYW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(sentences, batch_size, threshold):\n",
        "    \"\"\"Batch sentences, noisy sentences, and the lengths of their sentences together.\n",
        "       With each epoch, sentences will receive new mistakes\"\"\"\n",
        "    \n",
        "    for batch_i in range(0, len(sentences)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
        "        \n",
        "        sentences_batch_noisy = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n",
        "            \n",
        "        sentences_batch_eos = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentence.append(vocab_to_int['<EOS>'])\n",
        "            sentences_batch_eos.append(sentence)\n",
        "            \n",
        "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
        "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_sentences_lengths = []\n",
        "        for sentence in pad_sentences_batch:\n",
        "            pad_sentences_lengths.append(len(sentence))\n",
        "        \n",
        "        pad_sentences_noisy_lengths = []\n",
        "        for sentence in pad_sentences_noisy_batch:\n",
        "            pad_sentences_noisy_lengths.append(len(sentence))\n",
        "        \n",
        "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzeYHHU7MYXA",
        "colab_type": "text"
      },
      "source": [
        "*Note: This set of values achieved the best results.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi4nuPIyMYXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The default parameters\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "num_layers = 2\n",
        "rnn_size = 512\n",
        "embedding_size = 128\n",
        "learning_rate = 0.0005\n",
        "direction = 2\n",
        "threshold = 0.95\n",
        "keep_probability = 0.75"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wun2-gADMYXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      inputs_length,\n",
        "                                                      targets_length,\n",
        "                                                      max_target_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size,\n",
        "                                                      embedding_size,\n",
        "                                                      direction)\n",
        "\n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "\n",
        "    with tf.name_scope('predictions'):\n",
        "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "        tf.summary.histogram('predictions', predictions)\n",
        "\n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
        "    \n",
        "    with tf.name_scope(\"cost\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n",
        "                                                targets, \n",
        "                                                masks)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "\n",
        "    with tf.name_scope(\"optimze\"):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "\n",
        "    # Merge all of the summaries\n",
        "    merged = tf.summary.merge_all()    \n",
        "\n",
        "    # Export the nodes \n",
        "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n",
        "                    'predictions', 'merged', 'train_op','optimizer']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "\n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "p3zAfBjUMYXG",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hnnBwjXzMYXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, epochs, log_string):\n",
        "    '''Train the RNN'''\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Used to determine when to stop the training early\n",
        "        testing_loss_summary = []\n",
        "\n",
        "        # Keep track of which batch iteration is being trained\n",
        "        iteration = 0\n",
        "        \n",
        "        display_step = 30 # The progress of the training will be displayed after every 30 batches\n",
        "        stop_early = 0 \n",
        "        stop = 3 # If the batch_loss_testing does not decrease in 3 consecutive checks, stop training\n",
        "        per_epoch = 3 # Test the model 3 times per epoch\n",
        "        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n",
        "\n",
        "        print()\n",
        "        print(\"Training Model: {}\".format(log_string))\n",
        "\n",
        "        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n",
        "        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n",
        "\n",
        "        for epoch_i in range(1, epochs+1): \n",
        "            batch_loss = 0\n",
        "            batch_time = 0\n",
        "            \n",
        "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
        "                    get_batches(training_sorted, batch_size, threshold)):\n",
        "                start_time = time.time()\n",
        "\n",
        "                summary, loss, _ = sess.run([model.merged,\n",
        "                                             model.cost, \n",
        "                                             model.train_op], \n",
        "                                             {model.inputs: input_batch,\n",
        "                                              model.targets: target_batch,\n",
        "                                              model.inputs_length: input_length,\n",
        "                                              model.targets_length: target_length,\n",
        "                                              model.keep_prob: keep_probability})\n",
        "\n",
        "\n",
        "                batch_loss += loss\n",
        "                end_time = time.time()\n",
        "                batch_time += end_time - start_time\n",
        "\n",
        "                # Record the progress of training\n",
        "                train_writer.add_summary(summary, iteration)\n",
        "\n",
        "                iteration += 1\n",
        "\n",
        "                if batch_i % display_step == 0 and batch_i > 0:\n",
        "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(epoch_i,\n",
        "                                  epochs, \n",
        "                                  batch_i, \n",
        "                                  len(training_sorted) // batch_size, \n",
        "                                  batch_loss / display_step, \n",
        "                                  batch_time))\n",
        "                    batch_loss = 0\n",
        "                    batch_time = 0\n",
        "\n",
        "                #### Testing ####\n",
        "                if batch_i % testing_check == 0 and batch_i > 0:\n",
        "                    batch_loss_testing = 0\n",
        "                    batch_time_testing = 0\n",
        "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
        "                            get_batches(testing_sorted, batch_size, threshold)):\n",
        "                        start_time_testing = time.time()\n",
        "                        summary, loss = sess.run([model.merged,\n",
        "                                                  model.cost], \n",
        "                                                     {model.inputs: input_batch,\n",
        "                                                      model.targets: target_batch,\n",
        "                                                      model.inputs_length: input_length,\n",
        "                                                      model.targets_length: target_length,\n",
        "                                                      model.keep_prob: 1})\n",
        "\n",
        "                        batch_loss_testing += loss\n",
        "                        end_time_testing = time.time()\n",
        "                        batch_time_testing += end_time_testing - start_time_testing\n",
        "\n",
        "                        # Record the progress of testing\n",
        "                        test_writer.add_summary(summary, iteration)\n",
        "\n",
        "                    n_batches_testing = batch_i + 1\n",
        "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(batch_loss_testing / n_batches_testing, \n",
        "                                  batch_time_testing))\n",
        "                    \n",
        "                    batch_time_testing = 0\n",
        "\n",
        "                    # If the batch_loss_testing is at a new minimum, save the model\n",
        "                    testing_loss_summary.append(batch_loss_testing)\n",
        "                    if batch_loss_testing <= min(testing_loss_summary):\n",
        "                        print('New Record!') \n",
        "                        stop_early = 0\n",
        "                        checkpoint = \"./{}.ckpt\".format(log_string)\n",
        "                        saver = tf.train.Saver()\n",
        "                        saver.save(sess, checkpoint)\n",
        "\n",
        "                    else:\n",
        "                        print(\"No Improvement.\")\n",
        "                        stop_early += 1\n",
        "                        if stop_early == stop:\n",
        "                            break\n",
        "\n",
        "            if stop_early == stop:\n",
        "                print(\"Stopping Training.\")\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-lJ5q34vMYXI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3437
        },
        "outputId": "37275f02-eef9-4167-c6d0-a256ef6b71cd"
      },
      "source": [
        "# Train the model with the desired tuning parameters\n",
        "for keep_probability in [0.75]:\n",
        "    for num_layers in [2]:\n",
        "        for threshold in [0.95]:\n",
        "            log_string = 'kp={},nl={},th={}'.format(keep_probability,\n",
        "                                                    num_layers,\n",
        "                                                    threshold) \n",
        "            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n",
        "                                learning_rate, embedding_size, direction)\n",
        "            train(model, epochs, log_string)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-23-dce37f4df695>:25: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-23-dce37f4df695>:37: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "\n",
            "Training Model: kp=0.75,nl=2,th=0.95\n",
            "Epoch   1/100 Batch   30/909 - Loss:  3.113, Seconds: 4.45\n",
            "Epoch   1/100 Batch   60/909 - Loss:  1.920, Seconds: 3.52\n",
            "Epoch   1/100 Batch   90/909 - Loss:  1.309, Seconds: 4.40\n",
            "Epoch   1/100 Batch  120/909 - Loss:  0.861, Seconds: 5.64\n",
            "Epoch   1/100 Batch  150/909 - Loss:  0.608, Seconds: 6.16\n",
            "Epoch   1/100 Batch  180/909 - Loss:  0.749, Seconds: 7.07\n",
            "Epoch   1/100 Batch  210/909 - Loss:  0.692, Seconds: 8.19\n",
            "Epoch   1/100 Batch  240/909 - Loss:  0.415, Seconds: 9.84\n",
            "Epoch   1/100 Batch  270/909 - Loss:  0.352, Seconds: 10.48\n",
            "Epoch   1/100 Batch  300/909 - Loss:  0.328, Seconds: 11.77\n",
            "Testing Loss:  3.480, Seconds: 77.03\n",
            "New Record!\n",
            "Epoch   1/100 Batch  330/909 - Loss:  1.908, Seconds: 13.31\n",
            "Epoch   1/100 Batch  360/909 - Loss:  0.988, Seconds: 14.84\n",
            "Epoch   1/100 Batch  390/909 - Loss:  0.426, Seconds: 16.87\n",
            "Epoch   1/100 Batch  420/909 - Loss:  0.324, Seconds: 18.67\n",
            "Epoch   1/100 Batch  450/909 - Loss:  0.298, Seconds: 20.50\n",
            "Epoch   1/100 Batch  480/909 - Loss:  0.278, Seconds: 22.78\n",
            "Epoch   1/100 Batch  510/909 - Loss:  0.266, Seconds: 25.89\n",
            "Epoch   1/100 Batch  540/909 - Loss:  0.258, Seconds: 28.55\n",
            "Epoch   1/100 Batch  570/909 - Loss:  0.244, Seconds: 31.60\n",
            "Epoch   1/100 Batch  600/909 - Loss:  0.234, Seconds: 35.87\n",
            "Testing Loss:  1.924, Seconds: 88.94\n",
            "New Record!\n",
            "Epoch   1/100 Batch  630/909 - Loss:  0.811, Seconds: 39.99\n",
            "Epoch   1/100 Batch  660/909 - Loss:  0.396, Seconds: 43.62\n",
            "Epoch   1/100 Batch  690/909 - Loss:  0.288, Seconds: 49.21\n",
            "Epoch   1/100 Batch  720/909 - Loss:  0.265, Seconds: 54.54\n",
            "Epoch   1/100 Batch  750/909 - Loss:  0.267, Seconds: 62.05\n",
            "Epoch   1/100 Batch  780/909 - Loss:  0.248, Seconds: 70.29\n",
            "Epoch   1/100 Batch  810/909 - Loss:  0.242, Seconds: 80.93\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,216,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node decode/Training_Decoder/decoder/while/BasicDecoderStep/decoder/attention_wrapper/bahdanau_attention/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node decode_1/Inference_Decoder/decoder/while/Switch_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e588cb832a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n\u001b[1;32m      8\u001b[0m                                 learning_rate, embedding_size, direction)\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-cabe96074e2b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, log_string)\u001b[0m\n\u001b[1;32m     38\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                               \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                                               model.keep_prob: keep_probability})\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,216,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node decode/Training_Decoder/decoder/while/BasicDecoderStep/decoder/attention_wrapper/bahdanau_attention/mul (defined at <ipython-input-24-d4eb6545befc>:18) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node decode_1/Inference_Decoder/decoder/while/Switch_3 (defined at <ipython-input-25-075d29a8ed1b>:20) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'decode/Training_Decoder/decoder/while/BasicDecoderStep/decoder/attention_wrapper/bahdanau_attention/mul', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-e588cb832a2f>\", line 8, in <module>\n    learning_rate, embedding_size, direction)\n  File \"<ipython-input-31-793bc0409db3>\", line 21, in build_graph\n    direction)\n  File \"<ipython-input-27-19a6b98f1c37>\", line 27, in seq2seq_model\n    direction)\n  File \"<ipython-input-26-39c0373805fd>\", line 40, in decoding_layer\n    max_target_length)\n  File \"<ipython-input-24-d4eb6545befc>\", line 18, in training_decoding_layer\n    maximum_iterations=max_target_length)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 322, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3556, in while_loop\n    return_same_structure)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3087, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3022, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3525, in <lambda>\n    body = lambda i, lv: (i + 1, orig_body(*lv))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 265, in body\n    decoder_finished) = decoder.step(time, inputs, state)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 137, in step\n    cell_outputs, cell_state = self._cell(inputs, state)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 234, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\", line 530, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 554, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 1448, in call\n    self._attention_layers[i] if self._attention_layers else None)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 1054, in _compute_attention\n    cell_output, state=attention_state)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 581, in __call__\n    score = _bahdanau_score(processed_query, self._keys, self._normalize)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 490, in _bahdanau_score\n    return math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query), [2])\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 935, in _run_op\n    return tensor_oper(a.value(), *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 812, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 1078, in _mul_dispatch\n    return gen_math_ops.mul(x, y, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 5860, in mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,216,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node decode/Training_Decoder/decoder/while/BasicDecoderStep/decoder/attention_wrapper/bahdanau_attention/mul (defined at <ipython-input-24-d4eb6545befc>:18) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node decode_1/Inference_Decoder/decoder/while/Switch_3 (defined at <ipython-input-25-075d29a8ed1b>:20) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "7JRVPjsGMYXL",
        "colab_type": "text"
      },
      "source": [
        "## Fixing Custom Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQzouPWSMYXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_ints(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int[word] for word in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQDCpFnDMYXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your own sentence or use one from the dataset\n",
        "text = \"Spellin is difficult, correcting speling is mukh more difikult.\"\n",
        "text = text_to_ints(text)\n",
        "\n",
        "#random = np.random.randint(0,len(testing_sorted))\n",
        "#text = testing_sorted[random]\n",
        "#text = noise_maker(text, 0.95)\n",
        "\n",
        "checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Load saved model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, checkpoint)\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
        "                                                 model.inputs_length: [len(text)]*batch_size,\n",
        "                                                 model.targets_length: [len(text)+1], \n",
        "                                                 model.keep_prob: [1.0]})[0]\n",
        "\n",
        "# Remove the padding from the generated sentence\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('\\nText')\n",
        "print('  Word Ids:    {}'.format([i for i in text]))\n",
        "print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
        "print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "3Rqu9T5UMYXd",
        "colab_type": "text"
      },
      "source": [
        "Examples of corrected sentences:\n",
        "- Spellin is difficult, whch is wyh you need to study everyday.\n",
        "- Spelling is difficult, which is why you need to study everyday.\n",
        "\n",
        "\n",
        "- The first days of her existence in th country were vrey hard for Dolly. \n",
        "- The first days of her existence in the country were very hard for Dolly.\n",
        "\n",
        "\n",
        "- Thi is really something impressiv thaat we should look into right away! \n",
        "- This is really something impressive that we should look into right away!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "JahBcxMoMYXe",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdqoXpPmMYXg",
        "colab_type": "text"
      },
      "source": [
        "I hope that you have found this project to be rather interesting and useful. The example sentences that I have presented above were specifically chosen, and the model will not always be able to make corrections of this quality. Given the amount of data that we are working with, this model still struggles. For it to be more useful, it would require far more training data, and additional parameter tuning. This parameter values that I have above worked best for me, but I expect there are even better values that I was not able to find.\n",
        "\n",
        "Thanks for reading!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvbY27QHvNjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}